{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo 2: Aplicación de Algoritmos Genéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de curvas con AG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a8/Regression_pic_assymetrique.gif\" width=\"400px\" height=\"125px\" />\n",
    "\n",
    "> El **ajuste de curvas** es el proceso de construir una curva (función), que sea el mejor ajuste a una serie de puntos. Las curvas ajustadas pueden ser usadas como asistencia en la visualización de datos, para inferir valores de una función donde no hay datos disponibles, y para resumir la relación entre variables.\n",
    "\n",
    "**Referencia**:\n",
    "- https://en.wikipedia.org/wiki/Curve_fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\" width=\"400px\" height=\"125px\" />\n",
    "\n",
    "Consideramos que tenemos un conjunto de n pares ordenados de datos $(x_i,y_i)$, para $i=1,2,3,\\dots,n$.\n",
    "\n",
    "### ¿Cuál es la recta que mejor se ajusta a estos datos?\n",
    "Consideramos entonces ajustes de la forma $\\hat{f}(x) = \\beta_0+\\beta_1 x = \\left[1 \\quad x\\right]\\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right]=\\left[1 \\quad x\\right]\\boldsymbol{\\beta}$ (lineas rectas).\n",
    "\n",
    "Para decir '*mejor*', tenemos que definir algún sentido en que una recta se ajuste *mejor* que otra.\n",
    "\n",
    "**Mínimos cuadrados**: el objetivo es seleccionar los coeficientes $\\boldsymbol{\\beta}=\\left[\\beta_0 \\quad \\beta_1 \\right]^T$, de forma que la función evaluada en los puntos $x_i$ ($\\hat{f}(x_i)$) aproxime los valores correspondientes $y_i$.\n",
    "\n",
    "La formulación por mínimos cuadrados, encuentra los $\\boldsymbol{\\beta}=\\left[\\beta_0 \\quad \\beta_1 \\right]^T$ que minimiza\n",
    "$$\\frac{1}{2n}\\sum_{i=1}^{n}(y_i-\\hat{f}(x_i))^2=\\frac{1}{2n}\\sum_{i=1}^{n}(y_i-(\\beta_0+ \\beta_1x_i))^2=\\frac{1}{2n}\\sum_{i=1}^{n}(y_i-\\left[1 \\quad x_i\\right]\\boldsymbol{\\beta})^2=\\frac{1}{2n}\\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2,$$\n",
    "\n",
    "donde $\\boldsymbol{y}=\\left[y_1\\quad\\dots\\quad y_n\\right]^T$, y $\\boldsymbol{X}=\\left[\\begin{array}{ccc}1 & x_1\\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}\\right].$ Esto es,\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{ls} = \\arg \\min_{\\boldsymbol{\\beta}} \\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un conjunto de puntos ruidosos a partir de una recta\n",
    "N=100\n",
    "x=np.linspace(0,10,N)\n",
    "y = 10 +2*x+np.random.normal(loc=0, scale=2, size=(N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficar\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr',label='datos')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el módulo optimize de la librería scipy\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module scipy.optimize.optimize in scipy.optimize:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    #__docformat__ = \"restructuredtext en\"\n",
      "    # ******NOTICE***************\n",
      "    # optimize.py module by Travis E. Oliphant\n",
      "    #\n",
      "    # You may copy and use this module as you see fit with no\n",
      "    # guarantee implied provided you keep this notice in all copies.\n",
      "    # *****END NOTICE************\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        OptimizeWarning\n",
      "    builtins.dict(builtins.object)\n",
      "        OptimizeResult\n",
      "    \n",
      "    class OptimizeResult(builtins.dict)\n",
      "     |  Represents the optimization result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun, jac, hess: ndarray\n",
      "     |      Values of objective function, its Jacobian and its Hessian (if\n",
      "     |      available). The Hessians may be approximations, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There may be additional attributes not listed above depending of the\n",
      "     |  specific solver. Since this class is essentially a subclass of dict\n",
      "     |  with attribute accessors, one can see which attributes are available\n",
      "     |  using the `keys()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the dict keys.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(self, /)\n",
      "     |      Remove and return a (key, value) pair as a 2-tuple.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      "     |      Raises KeyError if the dict is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  Base class for warnings generated by user code.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    approx_fprime(xk, f, epsilon, *args)\n",
      "        Finite-difference approximation of the gradient of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            The function of which to determine the gradient (partial derivatives).\n",
      "            Should take `xk` as first argument, other arguments to `f` can be\n",
      "            supplied in ``*args``.  Should return a scalar, the value of the\n",
      "            function at `xk`.\n",
      "        epsilon : array_like\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives.  If an array, should contain one value per element of\n",
      "            `xk`.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        grad : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "        \n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "        \n",
      "        The main use of `approx_fprime` is in scalar function optimizers like\n",
      "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "        \n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004198])\n",
      "    \n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of the function.\n",
      "        \n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initital points) and return\n",
      "        new points xa, xb, xc that bracket the minimum of the function\n",
      "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
      "        solution will satisfy xa<=x<=xb\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values in bracket.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "    \n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one-variable and a possible bracket, return\n",
      "        the local minimum of the function isolated to a fractional precision\n",
      "        of tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple (xa,xb,xc) where xa<xb<xc and func(xb) <\n",
      "            func(xa), func(xc) or a pair (xa,xb) which are used as a\n",
      "            starting interval for a downhill bracket search (see\n",
      "            `bracket`). Providing the pair (xa,xb) does not always mean\n",
      "            the obtained solution will satisfy xa<=x<=xb.\n",
      "        tol : float, optional\n",
      "            Stop if between iteration change is less than `tol`.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            Optimum value.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of objective function evaluations made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "        \n",
      "        Does not ensure that the minimum lies in the range specified by\n",
      "        `brack`. See `fminbound`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range (xa,xb).\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.brent(f,brack=(1,2))\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.brent(f,brack=(-1,0.5,2))\n",
      "        >>> minimum\n",
      "        -2.7755575615628914e-17\n",
      "    \n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x0000023DC0B6F9D0>, disp=False, workers=1)\n",
      "        Minimize a function over a given range by brute force.\n",
      "        \n",
      "        Uses the \"brute force\" method, i.e. computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "        \n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function.  The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "        \n",
      "        The brute force approach is inefficient because the number of grid points\n",
      "        increases exponentially - the number of grid points to evaluate is\n",
      "        ``Ns ** len(x)``. Consequently, even with coarse grid spacing, even\n",
      "        moderately sized problems can take a long time to run, and/or run into\n",
      "        memory limitations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess.  `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments.  It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments.  Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages from the `finish` callable.\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the grid is subdivided into `workers`\n",
      "            sections and evaluated in parallel (uses\n",
      "            `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply `-1` to use all cores available to the Process.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the grid in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.3.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid.  It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs.  If `finish` is None, that is the\n",
      "        point returned.  When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "        \n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that.  Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "        \n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "        \n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in `finish=None`.\n",
      "        \n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
      "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "        \n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "        \n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "        \n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "        \n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "        \n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed.  To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "        \n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "        \n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "    \n",
      "    check_grad(func, grad, x0, *args, **kwargs)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Gradient of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(numpy.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        approx_fprime\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08\n",
      "    \n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "        \n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the j-th vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "        \n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin(f, 1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 17\n",
      "                 Function evaluations: 34\n",
      "        >>> minimum[0]\n",
      "        -8.8817841970012523e-16\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "        \n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "    \n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable f'(x,*args), optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Gradient norm must be less than gtol before successful termination.\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If fprime is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration.  Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
      "            in addition to xopt.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "            3 : NaN result encountered.\n",
      "        allvecs  :  list\n",
      "            The value of xopt at each iteration.  Only returned if retall is True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'BFGS' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, f, whose gradient is given by fprime\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
      "    \n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made.  Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "        \n",
      "            0 : Success.\n",
      "        \n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "        \n",
      "            2 : Gradient and/or function calls were not changing.  May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "        \n",
      "            3 : NaN result encountered.\n",
      "        \n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions.  It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "        \n",
      "        Conjugate gradient methods tend to work better when:\n",
      "        \n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "        \n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "        \n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "        \n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "    \n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration.  Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Line search failure (precision loss).\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "        \n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "        \n",
      "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
      "    \n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method.\n",
      "        \n",
      "        This method only uses function values, not derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, ``fopt``, ``xi``, ``direc``, ``iter``, ``funcalls``, and\n",
      "            ``warnflag`` are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial fitting step and parameter order set as an (N, N) array, where N\n",
      "            is the number of fitting parameters in `x0`.  Defaults to step size 1.0\n",
      "            fitting all parameters simultaneously (``np.ones((N, N))``).  To\n",
      "            prevent initial consideration of values in a step or to change initial\n",
      "            step size, set to 0 or desired step size in the Jth position in the Mth\n",
      "            block, where J is the position in `x0` and M is the desired evaluation\n",
      "            step, with steps being evaluated in index order.  Step size and ordering\n",
      "            will change freely as minimization proceeds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "                3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' method in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "        \n",
      "        The algorithm has two loops.  The outer loop merely iterates over the inner\n",
      "        loop. The inner loop minimizes over each current direction in the direction\n",
      "        set. At the end of the inner loop, if certain conditions are met, the\n",
      "        direction that gave the largest decrease is dropped and replaced with the\n",
      "        difference between the current estimated x and the estimated x from the\n",
      "        beginning of the inner-loop.\n",
      "        \n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "        \n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "        \n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin_powell(f, -1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 2\n",
      "                 Function evaluations: 18\n",
      "        >>> minimum\n",
      "        array(0.0)\n",
      "    \n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            The optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp : int, optional\n",
      "            If non-zero, print messages.\n",
      "                0 : no message printing.\n",
      "                1 : non-convergence notification messages only.\n",
      "                2 : print a message on convergence too.\n",
      "                3 : print iteration results.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            The function value at the minimum point.\n",
      "        ierr : int\n",
      "            An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "          The number of function calls made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
      "        for auto-bracketing).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        `fminbound` finds the minimum of the function in the given range.\n",
      "        The following examples illustrate the same\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fminbound(f, -1, 2)\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.fminbound(f, 1, 2)\n",
      "        >>> minimum\n",
      "        1.0000059608609866\n",
      "    \n",
      "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0, maxiter=5000)\n",
      "        Return the minimum of a function of one variable using golden section\n",
      "        method.\n",
      "        \n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
      "            func(a),func(c).  If bracket consists of two numbers (a,\n",
      "            c), then they are assumed to be a starting interval for a\n",
      "            downhill bracket search (see `bracket`); it doesn't always\n",
      "            mean that obtained solution will satisfy a<=x<=c.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.golden(f, brack=(1, 2))\n",
      "        >>> minimum\n",
      "        1.5717277788484873e-162\n",
      "        >>> minimum = optimize.golden(f, brack=(-1, 0.5, 2))\n",
      "        >>> minimum\n",
      "        -1.5717277788484873e-162\n",
      "    \n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        extra_condition : callable, optional\n",
      "            A callable of the form ``extra_condition(alpha, x, f, g)``\n",
      "            returning a boolean. Arguments are the proposed step ``alpha``\n",
      "            and the corresponding ``x``, ``f`` and ``g`` values. The line search \n",
      "            accepts the value of ``alpha`` only if this \n",
      "            callable returns ``True``. If the callable returns ``False`` \n",
      "            for the step length, the algorithm will continue with \n",
      "            new iterates. The callable is only called for iterates \n",
      "            satisfying the strong Wolfe conditions.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pg. 59-60.\n",
      "        \n",
      "        For the zoom phase it uses an algorithm by [...].\n",
      "    \n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "        \n",
      "        The function computed is::\n",
      "        \n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen_der, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen\n",
      "        >>> X = 0.1 * np.arange(10)\n",
      "        >>> rosen(X)\n",
      "        76.56\n",
      "    \n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_der\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> rosen_der(X)\n",
      "        array([ -2. ,  10.6,  15.6,  13.4,   6.4,  -3. , -12.4, -19.4,  62. ])\n",
      "    \n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess\n",
      "        >>> X = 0.1 * np.arange(4)\n",
      "        >>> rosen_hess(X)\n",
      "        array([[-38.,   0.,   0.,   0.],\n",
      "               [  0., 134., -40.,   0.],\n",
      "               [  0., -40., 130., -80.],\n",
      "               [  0.,   0., -80., 200.]])\n",
      "    \n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess_prod\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> p = 0.5 * np.arange(9)\n",
      "        >>> rosen_hess_prod(X, p)\n",
      "        array([  -0.,   27.,  -10.,  -95., -192., -265., -278., -195., -180.])\n",
      "    \n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "        \n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', or 'linprog'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=True) or the text string (disp=False)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "        \n",
      "        `scipy.optimize.minimize`\n",
      "        \n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "        \n",
      "        `scipy.optimize.root`\n",
      "        \n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "        \n",
      "        `scipy.optimize.minimize_scalar`\n",
      "        \n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "        \n",
      "        `scipy.optimize.linprog`\n",
      "        \n",
      "        - :ref:`simplex         <optimize.linprog-simplex>`\n",
      "        - :ref:`interior-point  <optimize.linprog-interior-point>`\n",
      "\n",
      "DATA\n",
      "    __all__ = ['fmin', 'fmin_powell', 'fmin_bfgs', 'fmin_ncg', 'fmin_cg', ...\n",
      "    __docformat__ = 'restructuredtext en'\n",
      "\n",
      "FILE\n",
      "    c:\\users\\uie70742\\appdata\\roaming\\python\\python38\\site-packages\\scipy\\optimize\\optimize.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(opt.optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros importantes:\n",
    "- fun: función $f(x)$, se debe definir antes de llamar minimize, como `def f(x): ... return ...`\n",
    "- x0: valor inicial. En una función no lineal, en general, hay múltiples mínimos. Dependiendo de la semilla caerá en uno de esos mínimos. Se ingresa como $x0 = \\text{np.array}([x_{01},\\dots,x_{0n}])$.\n",
    "- bounds: como en linprog.\n",
    "- constraints: funciones que definen las restricciones $g_i(x)$ y $h_j(x)$. Se definen igual que $f(x)$ y se ingresan como {'ineq': g_i, 'eq': h_j}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir funcion objetivo y punto inicial\n",
    "def min_sqe(beta,x_points,y_points):\n",
    "    n = len(x_points)\n",
    "    f_hat = beta[0] + beta[1]*x_points\n",
    "    sqe = (1/(2*n))*((y_points - f_hat)**2).sum()\n",
    "    \n",
    "    return sqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_ini = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 2.671670858381932\n",
       " hess_inv: array([[ 3.93713589, -0.58757845],\n",
       "       [-0.58757845,  0.11753952]])\n",
       "      jac: array([ 0.00000000e+00, -5.96046448e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 32\n",
       "      nit: 7\n",
       "     njev: 8\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([9.78677128, 1.94479596])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solucion = opt.minimize(fun = min_sqe, x0=beta_ini, args=(x,y))\n",
    "solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.78677128, 1.94479596])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta = solucion.x\n",
    "Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficar\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr',label='datos')\n",
    "f_hat = Beta[0] + Beta[1]*x\n",
    "\n",
    "plt.plot(x,f_hat,'b', lw=3, label=f'Recta ajustada: $y=${np.round(Beta[0],2)} + {np.round(Beta[1],2)}$x$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo aplicariamos esto con AG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def rulette_wheel_selection(p):\n",
    "    c=np.cumsum(p)\n",
    "    r=sum(p)*np.random.rand()\n",
    "    \n",
    "    ind= np.argwhere(r<=c)\n",
    "    return ind[0][0]\n",
    "\n",
    "def crossover(p1,p2):\n",
    "    c1=copy.deepcopy(p1)\n",
    "    c2=copy.deepcopy(p2)\n",
    "    alpha = np.random.uniform(0,1,(c1['position'].shape))\n",
    "    c1['position']= alpha*p1['position'] + (1-alpha)*p2['position']\n",
    "    c2['position']= alpha*p2['position'] + (1-alpha)*p1['position']\n",
    "    return c1, c2\n",
    "\n",
    "def mutate(c, mu, sigma):\n",
    "    y = copy.deepcopy(c)\n",
    "    flag = np.random.rand(*(c['position'].shape)) <= mu\n",
    "    ind = np.argwhere(flag)\n",
    "    y['position'][ind] += sigma*np.random.rand(*ind.shape)\n",
    "    return y\n",
    "\n",
    "def bounds(c,varmin,varmax):\n",
    "    c['position'] = np.maximum(c['position'], varmin)\n",
    "    c['position'] = np.minimum(c['position'], varmax)\n",
    "    \n",
    "def sort(arr):\n",
    "    n=len(arr)\n",
    "    for i in range(n-1):\n",
    "        for j in range(0,n-i-1):\n",
    "            if arr[j]['cost'] > arr[j+1]['cost']:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(costfun, x_points, y_points, num_var, varmin, varmax, maxit, npop, num_children, mu, sigma, beta):\n",
    "    \n",
    "    #inicializar la población\n",
    "    population={}\n",
    "    for i in range(npop):\n",
    "        population[i] = {'position': None, 'cost':None} \n",
    "    \n",
    "    bestsol = copy.deepcopy(population)\n",
    "    bestsol_cost = np.inf\n",
    "        \n",
    "    for i in range(npop):\n",
    "        population[i]['position'] = np.random.uniform(varmin, varmax, num_var)\n",
    "        population[i]['cost'] = costfun(population[i]['position'],x_points, y_points,)\n",
    "        \n",
    "        if population[i]['cost'] < bestsol_cost:\n",
    "            bestsol = copy.deepcopy(population[i])\n",
    "    \n",
    "    print('best_sol: {}'.format(bestsol))\n",
    "    \n",
    "    bestcost = np.empty(maxit)\n",
    "    bestsolution = np.empty((maxit,num_var))\n",
    "    \n",
    "    for it in range(maxit):\n",
    "        #calcular las probabilidades de la ruleta\n",
    "        costs=[]\n",
    "        for i in range(len(population)):\n",
    "            costs.append(population[i]['cost'])\n",
    "        costs = np.array(costs)\n",
    "        avg_cost = np.mean(costs)\n",
    "\n",
    "        if avg_cost !=0:\n",
    "            costs = costs/avg_cost\n",
    "\n",
    "        props = np.exp(-beta*costs)\n",
    "        \n",
    "        for _ in range(num_children//2):\n",
    "            \n",
    "            # selección por ruleta\n",
    "            p1 = population[rulette_wheel_selection(props)]\n",
    "            p2 = population[rulette_wheel_selection(props)]\n",
    "            \n",
    "            # Crossover de los padres\n",
    "            c1, c2 = crossover(p1,p2)\n",
    "            \n",
    "            # Realizar la mutación\n",
    "            c1=mutate(c1,mu,sigma)\n",
    "            c2=mutate(c2,mu,sigma)\n",
    "            \n",
    "            bounds(c1, varmin, varmax)\n",
    "            bounds(c2, varmin, varmax)\n",
    "            \n",
    "            #evaluar la función de costo\n",
    "            c1['cost'] = costfun(c1['position'],x_points,y_points)\n",
    "            c2['cost'] = costfun(c2['position'],x_points,y_points)\n",
    "            \n",
    "            if type(bestsol_cost)==float:\n",
    "                if c1['cost']<bestsol_cost:\n",
    "                    bestsol_cost = copy.deepcopy(c1)\n",
    "            else:\n",
    "                if c1['cost']<bestsol_cost['cost']:\n",
    "                    bestsol_cost = copy.deepcopy(c1)\n",
    "\n",
    "            if c2['cost']<bestsol_cost['cost']:\n",
    "                bestsol_cost = copy.deepcopy(c2)\n",
    "                \n",
    "        #juntar la poblacion de la generación anterior con la nueva\n",
    "        population[len(population)] = c1\n",
    "        population[len(population)] = c2\n",
    "        \n",
    "        population = sort(population)\n",
    "        \n",
    "        #almacenar el history\n",
    "        bestcost[it] = bestsol_cost['cost']\n",
    "        bestsolution[it] = bestsol_cost['position']\n",
    "        \n",
    "        print('iteración {}, best_sol {}, best_cost {}'.format(it,bestsolution[it],bestcost[it]))\n",
    "        \n",
    "    out = population\n",
    "    return (out, bestsolution, bestcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un conjunto de puntos ruidosos a partir de una recta\n",
    "N=100\n",
    "x=np.linspace(0,10,N)\n",
    "y = 10 +2*x+np.random.normal(loc=0, scale=2, size=(N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficar\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr',label='datos')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sqe(beta,x_points,y_points):\n",
    "    n = len(x_points)\n",
    "    f_hat = beta[0] + beta[1]*x_points\n",
    "    sqe = (1/(2*n))*((y_points - f_hat)**2).sum()\n",
    "    \n",
    "    return sqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_sol: {'position': array([ 0.95575854, -1.24196564]), 'cost': 365.8109016254961}\n",
      "iteración 0, best_sol [8.41279165 1.6187947 ], best_cost 8.722079393128066\n",
      "iteración 1, best_sol [7.06289223 2.02241801], best_cost 5.4337367409439095\n",
      "iteración 2, best_sol [7.06289223 2.02241801], best_cost 5.4337367409439095\n",
      "iteración 3, best_sol [7.06289223 2.02241801], best_cost 5.4337367409439095\n",
      "iteración 4, best_sol [7.06289223 2.02241801], best_cost 5.4337367409439095\n",
      "iteración 5, best_sol [7.06289223 2.02241801], best_cost 5.4337367409439095\n",
      "iteración 6, best_sol [7.06289223 2.02241801], best_cost 5.4337367409439095\n",
      "iteración 7, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 8, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 9, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 10, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 11, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 12, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 13, best_sol [5.50536552 2.66553453], best_cost 3.240074705542123\n",
      "iteración 14, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 15, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 16, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 17, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 18, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 19, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 20, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 21, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 22, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 23, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 24, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 25, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 26, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 27, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 28, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 29, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 30, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 31, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 32, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 33, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 34, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 35, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 36, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 37, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 38, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 39, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 40, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 41, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 42, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 43, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 44, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 45, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 46, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 47, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 48, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 49, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 50, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 51, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 52, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 53, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 54, best_sol [8.53158046 2.51817496], best_cost 3.20863076133505\n",
      "iteración 55, best_sol [5.62294084 2.62031441], best_cost 3.1809905933681355\n",
      "iteración 56, best_sol [5.62294084 2.62031441], best_cost 3.1809905933681355\n",
      "iteración 57, best_sol [5.62294084 2.62031441], best_cost 3.1809905933681355\n",
      "iteración 58, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 59, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 60, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 61, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 62, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 63, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 64, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 65, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 66, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 67, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 68, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 69, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 70, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 71, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 72, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 73, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 74, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 75, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 76, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 77, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 78, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 79, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 80, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 81, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 82, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 83, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 84, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 85, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 86, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 87, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 88, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 89, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 90, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 91, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 92, best_sol [6.41173908 2.46207694], best_cost 2.73317600156961\n",
      "iteración 93, best_sol [8.13118117 2.29463349], best_cost 1.9438140068107754\n",
      "iteración 94, best_sol [8.13118117 2.29463349], best_cost 1.9438140068107754\n",
      "iteración 95, best_sol [8.13118117 2.29463349], best_cost 1.9438140068107754\n",
      "iteración 96, best_sol [8.13118117 2.29463349], best_cost 1.9438140068107754\n",
      "iteración 97, best_sol [8.13118117 2.29463349], best_cost 1.9438140068107754\n",
      "iteración 98, best_sol [8.13118117 2.29463349], best_cost 1.9438140068107754\n",
      "iteración 99, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 100, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 101, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 102, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 103, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 104, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 105, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 106, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 107, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 108, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 109, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 110, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 111, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 112, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 113, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 114, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 115, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 116, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 117, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 118, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 119, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 120, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 121, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 122, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 123, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 124, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 125, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 126, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 127, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 128, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 129, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 130, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 131, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 132, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 133, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 134, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 135, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteración 136, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 137, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 138, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 139, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 140, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 141, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 142, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 143, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 144, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 145, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 146, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 147, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 148, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 149, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 150, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 151, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 152, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 153, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 154, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 155, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 156, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 157, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 158, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 159, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 160, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 161, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 162, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 163, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 164, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 165, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 166, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 167, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 168, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 169, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 170, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 171, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 172, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 173, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 174, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 175, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 176, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 177, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 178, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 179, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 180, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 181, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 182, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 183, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 184, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 185, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 186, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 187, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 188, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 189, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 190, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 191, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 192, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 193, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 194, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 195, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 196, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 197, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 198, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 199, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 200, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 201, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 202, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 203, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 204, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 205, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 206, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 207, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 208, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 209, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 210, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 211, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 212, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 213, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 214, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 215, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 216, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 217, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 218, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 219, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 220, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 221, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 222, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 223, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 224, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 225, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 226, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 227, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 228, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 229, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 230, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 231, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 232, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 233, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 234, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 235, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 236, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 237, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 238, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 239, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 240, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 241, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 242, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 243, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 244, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 245, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteración 246, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 247, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 248, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 249, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 250, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 251, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 252, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 253, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 254, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 255, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 256, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 257, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 258, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 259, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 260, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 261, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 262, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 263, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 264, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 265, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 266, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 267, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 268, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 269, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 270, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 271, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 272, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 273, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 274, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 275, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 276, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 277, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 278, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 279, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 280, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 281, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 282, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 283, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 284, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 285, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 286, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 287, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 288, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 289, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 290, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 291, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 292, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 293, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 294, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 295, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 296, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 297, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 298, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 299, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 300, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 301, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 302, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 303, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 304, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 305, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 306, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 307, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 308, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 309, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 310, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 311, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 312, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 313, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 314, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 315, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 316, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 317, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 318, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 319, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 320, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 321, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 322, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 323, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 324, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 325, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 326, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 327, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 328, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 329, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 330, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 331, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 332, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 333, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 334, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 335, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 336, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 337, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 338, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 339, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 340, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 341, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 342, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 343, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 344, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 345, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 346, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 347, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 348, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 349, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 350, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 351, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 352, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 353, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 354, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteración 355, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 356, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 357, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 358, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 359, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 360, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 361, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 362, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 363, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 364, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 365, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 366, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 367, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 368, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 369, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 370, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 371, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 372, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 373, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 374, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 375, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 376, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 377, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 378, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 379, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 380, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 381, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 382, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 383, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 384, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 385, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 386, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 387, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 388, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 389, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 390, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 391, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 392, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 393, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 394, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 395, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 396, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 397, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 398, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 399, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 400, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 401, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 402, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 403, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 404, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 405, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 406, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 407, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 408, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 409, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 410, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 411, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 412, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 413, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 414, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 415, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 416, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 417, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 418, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 419, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 420, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 421, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 422, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 423, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 424, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 425, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 426, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 427, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 428, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 429, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 430, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 431, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 432, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 433, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 434, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 435, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 436, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 437, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 438, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 439, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 440, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 441, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 442, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 443, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 444, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 445, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 446, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 447, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 448, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 449, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 450, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 451, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 452, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 453, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 454, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 455, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 456, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 457, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 458, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 459, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 460, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteración 461, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 462, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 463, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 464, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 465, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 466, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 467, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 468, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 469, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 470, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 471, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 472, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 473, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 474, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 475, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 476, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 477, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 478, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 479, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 480, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 481, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 482, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 483, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 484, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 485, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 486, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 487, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 488, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 489, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 490, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 491, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 492, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 493, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 494, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 495, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 496, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 497, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 498, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 499, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n",
      "iteración 500, best_sol [8.54975286 2.20996227], best_cost 1.9116585596248599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem definition\n",
    "costfunc = min_sqe\n",
    "num_var = 2       # number of decicion variables\n",
    "varmin = -10      # lower bound\n",
    "varmax = 10       # upper bound\n",
    "\n",
    "# GA Parameters\n",
    "maxit = 501                                              # number of iterations\n",
    "npop = 20                                                # initial population size\n",
    "beta = 1\n",
    "prop_children = 1                                        # proportion of children to population\n",
    "num_children = int(np.round(prop_children * npop/2)*2)   # making sure it always an even number\n",
    "mu = 0.2                                                 # mutation rate 20%, 205 of 5 is 1, mutating 1 gene\n",
    "sigma = 0.1                                              # step size of mutation\n",
    "\n",
    "\n",
    "# Run GA\n",
    "out = ga(costfunc, x, y, num_var, varmin, varmax, maxit, npop, num_children, mu, sigma, beta)\n",
    "\n",
    "# Results\n",
    "#(out, Bestsol, bestcost)\n",
    "plt.plot(out[2])\n",
    "plt.xlim(0, maxit)\n",
    "plt.xlabel('Generations')\n",
    "plt.ylabel('Best Cost')\n",
    "plt.title('Genetic Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.54975286, 2.20996227])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_ag =out[1][-1]\n",
    "Beta_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficar\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr',label='datos')\n",
    "f_hat = Beta_ag[0] + Beta_ag[1]*x\n",
    "\n",
    "plt.plot(x,f_hat,'b', lw=3, label=f'Recta ajustada: $y=${np.round(Beta_ag[0],2)} + {np.round(Beta_ag[1],2)}$x$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
