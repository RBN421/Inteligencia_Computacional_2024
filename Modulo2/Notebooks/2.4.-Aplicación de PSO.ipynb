{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd98cff3",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo 2: Aplicación de PSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f3ec6",
   "metadata": {},
   "source": [
    "## Clasificación usando regresión logística optimizada por un pso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a254eb",
   "metadata": {},
   "source": [
    "La regresión logística es un algoritmo de aprendizaje automático que se utiliza para hacer predicciones para encontrar el valor de una variable dependiente, como la de predecir si un tumor es maligno o benigno, la clasificación del correo electrónico (spam o no spam) o la admisión a una universidad (admitido o no admitido), este algoritmo va aprendiendo de variables independientes (varias características relevantes para el problema).\n",
    "\n",
    "Por ejemplo, para clasificar un correo electrónico, el algoritmo utilizará las palabras del correo electrónico como características y, en función de eso, hará una predicción de si el correo electrónico es spam o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c73a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbc86fc3",
   "metadata": {},
   "source": [
    "### Codificando Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4e4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767470f4",
   "metadata": {},
   "source": [
    "Una regresión Linea puede representarse como:\n",
    "\n",
    "$$\\hat{f}(x) = \\beta^T X$$\n",
    "\n",
    "Aplicando la función sigmoide a la salida de la regresión logística:\n",
    "\n",
    "$$\\hat{f}(x) = \\sigma(\\beta^T X)$$\n",
    "\n",
    "Donde la función sigmoide está definida como:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "Entonces el modelo de regresión logística nos queda:\n",
    "\n",
    "$$\\hat{f}(x) = \\frac{1}{1+e^{-\\beta^T X}}$$\n",
    "\n",
    "\n",
    "Si la suma ponderada de las entradas es mayor que cero, la clase predicha es 1 y viceversa. Entonces, el límite de decisión que separa ambas clases se puede encontrar al establecer la suma ponderada de las entradas en 0.\n",
    "\n",
    "$$\n",
    "\\hat{f}(\\textbf{x}) = \\begin{cases}  \\textrm{>0.5, si  }  \\beta^T X \\geq 0 \\\\ \\\\  \\textrm{<0.5, si  } \\beta^T X \\leq 0\\\\ \\end{cases}\n",
    "$$\n",
    "\n",
    "#### Función de costo\n",
    "\n",
    "Al igual que la regresión lineal, se define una función de costo para nuestro modelo y el objetivo será minimizar el costo.\n",
    "\n",
    "$$\n",
    "cost = \\begin{cases}  -\\log{\\hat{f}(x)}, \\textrm{si  }  y=1 \\\\ \\\\  -\\log{(1 - \\hat{f}(x)}), \\textrm{si  }  y=0 \\\\ \\end{cases}\n",
    "$$\n",
    "\n",
    "#### Intuición de la función de costo\n",
    "\n",
    "Si la clase de los datos reales es 1 y el modelo predice 0, deberíamos penalizar al modelo y viceversa. Como se puede ver en la figura de abajo, para la gráfica $-\\log(\\hat{f}(x))$ cuando $\\hat{f}(x)$ se acerca a 1, el costo es 0 y cuando h(x) se acerca a 0, el costo es infinito (es decir, penalizamos fuertemente el modelo). De manera similar, para la gráfica $-\\log(1-\\hat{f}(x))$ cuando el valor real es 0 y el modelo predice 0, el costo es 0 y el costo se vuelve infinito cuando $\\hat{f}(x)$ se acerca a 1.\n",
    "\n",
    "<img style=\"float: center;;\" src='Figures/log_reg.png' width=\"400\" height=\"100\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733355c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e0c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb9187dd",
   "metadata": {},
   "source": [
    "Combinando las dos ecuaciones tenemos:\n",
    "\n",
    "$$cost(\\hat{f}(x),y) = -y\\log(\\hat{f}(x)) - (1-y)\\log(1-\\hat{f}(x))$$\n",
    "\n",
    "Entonces el costo de todos los puntos en los datos reales se puede calcular tomando el promedio del costo de todas las muestras:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283c8b3",
   "metadata": {},
   "source": [
    "$$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^m[y^{(i)}\\log\\hat{f}(x^{(i)}) + (1-y^{(i)})\\log(1 - \\hat{f}(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee43eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54324408",
   "metadata": {},
   "source": [
    "### Optimizando con el Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27bcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd19807c",
   "metadata": {},
   "source": [
    "$$f_{reg} = -25.11 + 10.64*x1 + 5.36*x2$$\n",
    "$$\\hat{f} = \\sigma(-25.11 + 10.64*x1 + 5.36*x2)$$\n",
    "\n",
    "$$\\hat{f}(x) = \\frac{1}{1+e^{-(-25.11 + 10.64*x1 + 5.36*x2)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb70633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335dbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3d68332",
   "metadata": {},
   "source": [
    "Como hay dos características en nuestro conjunto de datos, la ecuación lineal se puede representar mediante:\n",
    "\n",
    "$$\\hat{f}(x)=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "\n",
    "El límite de decisión se puede encontrar estableciendo la suma ponderada de las entradas en 0. Es decir igualar $\\hat{f}(x)=0$, entonces:\n",
    "\n",
    "$$x_2 = - \\frac{\\beta_0 + \\beta_1 x_1}{\\beta_2}$$\n",
    "\n",
    "0\n",
    "\n",
    "$$x_1 = - \\frac{\\beta_0 + \\beta_2 x_2}{\\beta_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec6ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791037a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4514ebab",
   "metadata": {},
   "source": [
    "### Con Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(fit_intercept=False)\n",
    "model.fit(Xa, y.ravel())\n",
    "predict_class = model.predict(Xa)\n",
    "\n",
    "Beta_sklearn = model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [np.min(Xa[:,2]) -5, np.max(Xa[:,2])+5]\n",
    "y_values = - (Beta_sklearn[0] + np.dot(Beta_sklearn[2],x_values)) / Beta_sklearn[1]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X.iloc[:,0].values, X.iloc[:,1].values, c=y)\n",
    "plt.plot(x_values, y_values, label='frontera de decisión')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d2d23",
   "metadata": {},
   "source": [
    "## Aplicando PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bcd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b9599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
