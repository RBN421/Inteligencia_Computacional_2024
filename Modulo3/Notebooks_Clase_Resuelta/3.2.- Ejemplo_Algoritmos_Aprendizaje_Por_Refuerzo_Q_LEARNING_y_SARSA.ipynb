{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "# <center> <font color= #000047> Módulo 3: Algoritmos de Aprendizaje por Refuerzo: Q-Learning y SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este notebook vamos a resolver un problema con Aprendizaje por refuerzo usando los algoritmos del Q-Learning y SARSA-Learning. El problema que queremos resolver es el de encontrar el camino que nos suponga una mayor recompensa (el más corto) desde un estado inicial $[0,0]$ hasta el estado final $[4,4]$, pudiendo realizar 4 tipos de acciones:\n",
    "\n",
    "\n",
    "<img src=\"./Figures/007_RL.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "* Para resolver este problema vamos a realizar lo siguiente:\n",
    "<span></span><br>\n",
    "    1. [Definición del entorno](#M1)\n",
    "<span></span><br>\n",
    "    2. [Implementación de un algoritmo de toma aleatoria de acciones](#M2)\n",
    "<span></span><br>\n",
    "    3. [Ejecución: Entorno - Agente](#M3)\n",
    "<span></span><br>\n",
    "    4. [Q-Learner: Implementación y Ejecución](#M4)\n",
    "<span></span><br>\n",
    "    5. [SARSA-Learner: Implementación y Ejecución](#M5)\n",
    "<span></span><br>\n",
    "    6. [Estrategias a corto y largo plazo](#M5)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## Definición del entorno\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r = np.array([[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0]])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Formato de los decimales en Pandas y la semilla del Random\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.random.seed(23)\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, action_penalty=-1.0):\n",
    "        \"\"\"\n",
    "        Clase que representa y controla en entorno\n",
    "        :param action_penalty:    Factor de descuento del Reward por acción tomada\n",
    "        \"\"\"\n",
    "        self.action_penalty = action_penalty\n",
    "        #Accciones que puede tomar el agente\n",
    "        self.actions = {'Arriba': [0,-1],\n",
    "                        'Abajo': [0, 1],\n",
    "                        'Izquierda': [-1,0],\n",
    "                        'Derecha': [1, 0]}\n",
    "        self.state = [0,0]\n",
    "        self.final_state = [3,3]\n",
    "        self.total_reward = 0.0\n",
    "        self.actions_done = []\n",
    "        self.rewards = [[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0]]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Método que reinicia las variables del entorno y devuelve es estado inicial\n",
    "        :return:    state\n",
    "        \"\"\"\n",
    "        self.total_reward=0.0\n",
    "        self.state = [0,0]\n",
    "        self.actions_done = []\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Método que ejecuta una acción determinada del conjunto de acciones {Arriba, Abajo, Izquierda, Derecha}\n",
    "        para guiar al agente en el entorno.\n",
    "        :param action:    Acción a ejecutar\n",
    "        :return:          (state, reward, is_final_state)\n",
    "        \"\"\"\n",
    "        self.__apply_action(action)    #realizamos la acción para el cambio del destado\n",
    "        self.actions_done.append(self.state[:]) #Guardar el paso (acción realizada)\n",
    "        is_final_state = np.array_equal(self.state, self.final_state)\n",
    "        reward = self.reward[self.state[0]][self.state[1]] + self.action_penalty #Se calcula la recompensa por la acción que se tome\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        return self.state, reward, is_final_state\n",
    "\n",
    "    def __apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Método que calcula el nuevo estado a partir de la acción a ejecutar\n",
    "        :param action:    Acción a ejecutar\n",
    "        \"\"\"\n",
    "        self.state[0] += self.actions[action][0]  # Estado siguien S' para coordenada en x\n",
    "        self.state[1] += self.actions[action][1]  # Estado siguien S' para coordenada en y\n",
    "        \n",
    "        #Si nos salimos del tablero por arriba o por abajo, nos quedamos en la misma posición que estabamos\n",
    "        if self.state[0]<0:\n",
    "            self.state[0] = 0\n",
    "        elif self.state[0]>len(self.rewards) -1:\n",
    "            self.state[0] -= 1\n",
    "        \n",
    "        #Si nos salimos del tablero por arriba o por abajo, nos quedamos en la misma posición que estabamos\n",
    "        if self.state[1]<0:\n",
    "            self.state[1] = 0\n",
    "        elif self.state[1]>len(self.rewards[0]) -1:\n",
    "            self.state[1] -= 1\n",
    "        \n",
    "        \n",
    "    def print_path_episode(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el camino seguido por el agente\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        path = [['-' for _ in range(len(self.rewards))] for _ in range(len(self.rewards[0]))]\n",
    "        path[0][0] = '0'\n",
    "        for index, step in enumerate(self.actions_done):\n",
    "            path[step[0]][step[1]] = str(index + 1)\n",
    "        \n",
    "        print(pd.DataFrame(data= np.array([np.array(xi) for xi in path]), \n",
    "                                         index = [f\"x{str(i)}\" for in in range(len(path))], \n",
    "                                                 columns = [f\"y{str(i)}\" for i in range(len(path[0]))]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementación de un algoritmo de toma aleatoria de acciones\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "\n",
    "    def __init__(self, environment, learning_rate=0.1, discount_factor=0.1, ratio_exploration=0.05):\n",
    "        \"\"\"\n",
    "        Clase que implementa un algoritmo de aprendiza por refuerzo\n",
    "        Esta clase implementa un algoritmo de selección aleatoria de acciones\n",
    "        :param environment:         Entorno en el que tomar las acciones\n",
    "        :param learning_rate:       Factor de aprendizaje\n",
    "        :param discount_factor:     Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "        :param ratio_exploration:   Ratio de exploración\n",
    "        \"\"\"\n",
    "        self.enviroment = environment\n",
    "        self.q_table = [[[0.0 for _ in self.enviroment.actions]\n",
    "                        for _ in range(len(self.enviroment.rewards))]\n",
    "                       for _ in range(len(self.enviroment.rewards[0]))]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.ratio_exploration = ratio_exploration\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'random'\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        \"\"\"\n",
    "        Método que selecciona la siguiente acción a tomar:\n",
    "            Aleatoria -> si el ratio de exploración es inferior al umbral\n",
    "            Mejor Acción -> si el ratio de exploración es superior al umbral\n",
    "        :param state:   Estado del agente\n",
    "        :return:        next_action\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.uniform() < self.ratio_exploration:\n",
    "            #seleccionamos una acción al azar\n",
    "            next_action = np.random.choice(list(self.enviroment.actions))\n",
    "        else:\n",
    "            #seleccionamos la acción que nos da mayor valor. si hay empate seleccionamos al azar\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                self.q_table[state[0]][state[1]] == np.array(self.q_table[state[0]][state[1]]).max()\n",
    "            ))\n",
    "            nex_action = list(self.enviroment.actions)[idx_action]\n",
    "        \n",
    "        return next_action\n",
    "    \n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Actualiza la Q-Table\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def print_q_table(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la Q-Table\n",
    "        \"\"\"\n",
    "        q_table = []\n",
    "        \n",
    "        for x, row in enumerate(self.q_table):\n",
    "            for y, col in enumerate(row):\n",
    "                q = deepcopy(col)\n",
    "                q.insert(0,f'x{x},y{y}')\n",
    "                q_table.append(q)\n",
    "        \n",
    "        print(pd.DataFrame(data = q_table, columns = ['Estado', 'Arriba', 'Izquierda', 'Derecha']).to_string(index=False))\n",
    "        \n",
    "\n",
    "    def print_best_actions_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        best = [[list(self.enviroment.actions)[np.argmax(col)] for col in row] for row in self.q_table]\n",
    "        \n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index = [f'x{str(i)}' for i in range(len(best))],\n",
    "                          columns = [f'y{str(i)}' for i in range(len(best[0]))]))\n",
    "        \n",
    "        \n",
    "    def print_best_values_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el valor de la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        best = [[max(vi) for vi in row] for row in self.q_table]\n",
    "        \n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index = [f'x{str(i)}' for i in range(len(best))],\n",
    "                          columns = [f'y{str(i)}' for i in range(len(best[0]))]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejecución: Entorno - Agente\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def run_agent(learner=Learner, num_episodes=10, learning_rate=0.1, discount_factor=0.1, ratio_exploration=0.05,\n",
    "              verbose=False):\n",
    "    \"\"\"\n",
    "    Método que ejecuta el proceso de aprendizaje del agente en un entorno\n",
    "    :param learner:              Algoritmo de Aprendizaje\n",
    "    :param num_episodes:         Número de veces que se ejecuta (o aprende) el agente en el entorno\n",
    "    :param learning_rate:        Factor de Aprendizaje\n",
    "    :param discount_factor:      Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "    :param ratio_exploration:    Ratio de exploración\n",
    "    :param verbose:              Boolean, si queremos o no imprimir por pantalla información del proceso\n",
    "    :return:                     (episodes_list, best_episode)\n",
    "    \"\"\"\n",
    "\n",
    "    # Instanciamos el entorno\n",
    "   \n",
    "\n",
    "    # Instanciamos el método de aprendizaje\n",
    "   \n",
    "    # Variables para guardar la información de los episodios\n",
    "   \n",
    "        # Guardamos el mejor episodio\n",
    "        \n",
    "            # Imprimimos la información de los episodios\n",
    "            \n",
    "    return None, None\n",
    "\n",
    "\n",
    "def print_process_info(episodes_list, best_episode, print_best_episode_info=True,\n",
    "                       print_q_table=True, print_best_values_states=True,\n",
    "                       print_best_actions_states=True, print_steps=True, print_path=True):\n",
    "    \"\"\"\n",
    "    Método que imprime por pantalla los resultados de la ejecución\n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Política Aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Q-Learner: Implementación y Ejecución\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./Figures/013_qlearning.png\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'QLearner'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, is_final_state, **kwargs):\n",
    "        \"\"\"\n",
    "        Método que implementa el Algoritmo de aprendizaje del Q-Learning\n",
    "        :param environment:           Entorno en el que tomar las acciones\n",
    "        :param old_state:             Estado actual\n",
    "        :param action_taken:          Acción a realizar\n",
    "        :param reward_action_taken:   Recompensa obtenida por la acción tomada\n",
    "        :param new_state:             Nuevo estado al que se mueve el agente\n",
    "        :param is_final_state:        Boolean. Devuelve True si el agente llega al estado final; si no, False\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        # Obtenemos el identificador de la acción\n",
    "        \n",
    "\n",
    "        # Obtenemos el valor de la acción tomada\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a corto plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a largo plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## SARSA-Learner: Implementación y Ejecución\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./Figures/014_sarsa.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'SARSA'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, new_action, is_final_state):\n",
    "        \"\"\"\n",
    "        Método que implementa el algoritmo de aprendizaje SARSA\n",
    "        :param environment:           Entorno en el que tomar las acciones\n",
    "        :param old_state:             Estado actual\n",
    "        :param action_taken:          Acción a realizar\n",
    "        :param reward_action_taken:   Recompensa obtenida por la acción tomada\n",
    "        :param new_state:             Nuevo estado al que se mueve el agente \n",
    "        :param new_action:            Acción a tomar en el nuevo estado\n",
    "        :param is_final_state:        Boolean. Devuelve True si el agente llega al estado final; si no, False \n",
    "        \"\"\"\n",
    "        # Obtenemos el identificador de la acción\n",
    "        \n",
    "\n",
    "        # Obtenemos el valor de la acción tomada\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a corto plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a largo plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Estrategias a corto y largo plazo\n",
    "\n",
    "\n",
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
