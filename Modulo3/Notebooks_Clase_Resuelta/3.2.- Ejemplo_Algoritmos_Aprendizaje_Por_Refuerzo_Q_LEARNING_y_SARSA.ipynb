{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "# <center> <font color= #000047> Módulo 3: Algoritmos de Aprendizaje por Refuerzo: Q-Learning y SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este notebook vamos a resolver un problema con Aprendizaje por refuerzo usando los algoritmos del Q-Learning y SARSA-Learning. El problema que queremos resolver es el de encontrar el camino que nos suponga una mayor recompensa (el más corto) desde un estado inicial $[0,0]$ hasta el estado final $[4,4]$, pudiendo realizar 4 tipos de acciones:\n",
    "\n",
    "\n",
    "<img src=\"./Figures/007_RL.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "* Para resolver este problema vamos a realizar lo siguiente:\n",
    "<span></span><br>\n",
    "    1. [Definición del entorno](#M1)\n",
    "<span></span><br>\n",
    "    2. [Implementación de un algoritmo de toma aleatoria de acciones](#M2)\n",
    "<span></span><br>\n",
    "    3. [Ejecución: Entorno - Agente](#M3)\n",
    "<span></span><br>\n",
    "    4. [Q-Learner: Implementación y Ejecución](#M4)\n",
    "<span></span><br>\n",
    "    5. [SARSA-Learner: Implementación y Ejecución](#M5)\n",
    "<span></span><br>\n",
    "    6. [Estrategias a corto y largo plazo](#M5)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## Definición del entorno\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r = np.array([[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0]])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Formato de los decimales en Pandas y la semilla del Random\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.random.seed(23)\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, action_penalty=-1.0):\n",
    "        \"\"\"\n",
    "        Clase que representa y controla en entorno\n",
    "        :param action_penalty:    Factor de descuento del Reward por acción tomada\n",
    "        \"\"\"\n",
    "        self.action_penalty = action_penalty\n",
    "        #Accciones que puede tomar el agente\n",
    "        self.actions = {'Arriba': [0,-1],\n",
    "                        'Abajo': [0, 1],\n",
    "                        'Izquierda': [-1,0],\n",
    "                        'Derecha': [1, 0]}\n",
    "        self.state = [0,0]\n",
    "        self.final_state = [3,3]\n",
    "        self.total_reward = 0.0\n",
    "        self.actions_done = []\n",
    "        self.rewards = [[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0]]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Método que reinicia las variables del entorno y devuelve es estado inicial\n",
    "        :return:    state\n",
    "        \"\"\"\n",
    "        self.total_reward=0.0\n",
    "        self.state = [0,0]\n",
    "        self.actions_done = []\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Método que ejecuta una acción determinada del conjunto de acciones {Arriba, Abajo, Izquierda, Derecha}\n",
    "        para guiar al agente en el entorno.\n",
    "        :param action:    Acción a ejecutar\n",
    "        :return:          (state, reward, is_final_state)\n",
    "        \"\"\"\n",
    "        self.__apply_action(action)    #realizamos la acción para el cambio del destado\n",
    "        self.actions_done.append(self.state[:]) #Guardar el paso (acción realizada)\n",
    "        is_final_state = np.array_equal(self.state, self.final_state)\n",
    "        reward = self.rewards[self.state[0]][self.state[1]] + self.action_penalty #Se calcula la recompensa por la acción que se tome\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward, is_final_state\n",
    "\n",
    "    def __apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Método que calcula el nuevo estado a partir de la acción a ejecutar\n",
    "        :param action:    Acción a ejecutar\n",
    "        \"\"\"\n",
    "        self.state[0] += self.actions[action][0]  # Estado siguien S' para coordenada en x\n",
    "        self.state[1] += self.actions[action][1]  # Estado siguien S' para coordenada en y\n",
    "        \n",
    "        #Si nos salimos del tablero por arriba o por abajo, nos quedamos en la misma posición que estabamos\n",
    "        if self.state[0]<0:\n",
    "            self.state[0] = 0\n",
    "        elif self.state[0]>len(self.rewards) -1:\n",
    "            self.state[0] -= 1\n",
    "        \n",
    "        #Si nos salimos del tablero por arriba o por abajo, nos quedamos en la misma posición que estabamos\n",
    "        if self.state[1]<0:\n",
    "            self.state[1] = 0\n",
    "        elif self.state[1]>len(self.rewards[0]) -1:\n",
    "            self.state[1] -= 1\n",
    "        \n",
    "        \n",
    "    def print_path_episode(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el camino seguido por el agente\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        path = [['-' for _ in range(len(self.rewards))] for _ in range(len(self.rewards[0]))]\n",
    "        path[0][0] = '0'\n",
    "        for index, step in enumerate(self.actions_done):\n",
    "            path[step[0]][step[1]] = str(index + 1)\n",
    "        \n",
    "        print(pd.DataFrame(data= np.array([np.array(xi) for xi in path]), \n",
    "                                         index = [f\"x{str(i)}\" for i in range(len(path))], \n",
    "                                                 columns = [f\"y{str(i)}\" for i in range(len(path[0]))]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementación de un algoritmo de toma aleatoria de acciones\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "\n",
    "    def __init__(self, environment, learning_rate=0.1, discount_factor=0.1, ratio_exploration=0.05):\n",
    "        \"\"\"\n",
    "        Clase que implementa un algoritmo de aprendiza por refuerzo\n",
    "        Esta clase implementa un algoritmo de selección aleatoria de acciones\n",
    "        :param environment:         Entorno en el que tomar las acciones\n",
    "        :param learning_rate:       Factor de aprendizaje\n",
    "        :param discount_factor:     Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "        :param ratio_exploration:   Ratio de exploración\n",
    "        \"\"\"\n",
    "        self.enviroment = environment\n",
    "        self.q_table = [[[0.0 for _ in self.enviroment.actions]\n",
    "                        for _ in range(len(self.enviroment.rewards))]\n",
    "                       for _ in range(len(self.enviroment.rewards[0]))]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.ratio_exploration = ratio_exploration\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'random'\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        \"\"\"\n",
    "        Método que selecciona la siguiente acción a tomar:\n",
    "            Aleatoria -> si el ratio de exploración es inferior al umbral\n",
    "            Mejor Acción -> si el ratio de exploración es superior al umbral\n",
    "        :param state:   Estado del agente\n",
    "        :return:        next_action\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.uniform() < self.ratio_exploration:\n",
    "            #seleccionamos una acción al azar\n",
    "            next_action = np.random.choice(list(self.enviroment.actions))\n",
    "        else:\n",
    "            #seleccionamos la acción que nos da mayor valor. si hay empate seleccionamos al azar\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                self.q_table[state[0]][state[1]] == np.array(self.q_table[state[0]][state[1]]).max()\n",
    "            ))\n",
    "            next_action = list(self.enviroment.actions)[idx_action]\n",
    "\n",
    "        return next_action\n",
    "    \n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Actualiza la Q-Table\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def print_q_table(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la Q-Table\n",
    "        \"\"\"\n",
    "        q_table = []\n",
    "        \n",
    "        for x, row in enumerate(self.q_table):\n",
    "            for y, col in enumerate(row):\n",
    "                q = deepcopy(col)\n",
    "                q.insert(0,f'x{x},y{y}')\n",
    "                q_table.append(q)\n",
    "        \n",
    "        print(pd.DataFrame(data = q_table, columns = ['Estado', 'Arriba', 'Abajo', 'Izquierda', 'Derecha']).to_string(index=False))\n",
    "        \n",
    "\n",
    "    def print_best_actions_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        best = [[list(self.enviroment.actions)[np.argmax(col)] for col in row] for row in self.q_table]\n",
    "        \n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index = [f'x{str(i)}' for i in range(len(best))],\n",
    "                          columns = [f'y{str(i)}' for i in range(len(best[0]))]))\n",
    "        \n",
    "        \n",
    "    def print_best_values_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el valor de la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        best = [[max(vi) for vi in row] for row in self.q_table]\n",
    "        \n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index = [f'x{str(i)}' for i in range(len(best))],\n",
    "                          columns = [f'y{str(i)}' for i in range(len(best[0]))]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejecución: Entorno - Agente\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def run_agent(Learner=Learner, num_episodes=10, learning_rate=0.1, discount_factor=0.1, ratio_exploration=0.05,\n",
    "              verbose=False):\n",
    "    \"\"\"\n",
    "    Método que ejecuta el proceso de aprendizaje del agente en un entorno\n",
    "    :param learner:              Algoritmo de Aprendizaje\n",
    "    :param num_episodes:         Número de veces que se ejecuta (o aprende) el agente en el entorno\n",
    "    :param learning_rate:        Factor de Aprendizaje\n",
    "    :param discount_factor:      Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "    :param ratio_exploration:    Ratio de exploración\n",
    "    :param verbose:              Boolean, si queremos o no imprimir por pantalla información del proceso\n",
    "    :return:                     (episodes_list, best_episode)\n",
    "    \"\"\"\n",
    "\n",
    "    # Instanciamos el entorno\n",
    "    enviroment = Environment()\n",
    "\n",
    "    # Instanciamos el método de aprendizaje\n",
    "    learner = Learner(environment = enviroment,\n",
    "                      learning_rate =learning_rate, \n",
    "                      discount_factor = discount_factor,\n",
    "                      ratio_exploration = ratio_exploration)\n",
    "    \n",
    "    # Variables para guardar la información de los episodios\n",
    "    episodes_list = []\n",
    "    best_episode = None\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    for n_episode in range (0, num_episodes):\n",
    "        state = enviroment.reset()\n",
    "        reward = None\n",
    "        is_final_state = None\n",
    "        num_steps_episode = 0\n",
    "        while not is_final_state:\n",
    "            old_state = state[:]\n",
    "            next_action = learner.get_next_action(state = state)           #Acción a realizar en el estado actual \n",
    "            state, reward, is_final_state = enviroment.step(next_action)   # Realiza la acción por lo tanto pasamos al sig estado\n",
    "            next_pos_action = (learner.get_next_action(state) if learner.name == 'SARSA' else None)\n",
    "            \n",
    "            learner.update(environment = enviroment,                   #Actualizamos el entorno\n",
    "                          old_state = old_state,\n",
    "                          action_taken = next_action,\n",
    "                          reward_action_taken = reward,\n",
    "                          new_state = state,\n",
    "                          new_action = next_pos_action,\n",
    "                          is_final_state = is_final_state)\n",
    "            \n",
    "            num_steps_episode +=1                                     #Sumamos un paso al episodio\n",
    "        \n",
    "        episodes_list.append([n_episode + 1, num_steps_episode, enviroment.total_reward])  #Se guarda la información del episodio\n",
    "        \n",
    "        # Guardamos el mejor episodio\n",
    "        if enviroment.total_reward>= best_reward:\n",
    "            best_reward = enviroment.total_reward\n",
    "            best_episode = {'Num_episode': n_episode + 1, 'episode': deepcopy(enviroment), 'learner': deepcopy(learner) }\n",
    "            \n",
    "            # Imprimimos la información de los episodios\n",
    "        if verbose:\n",
    "            print(f'Episodio {n_episode +1}, Num Acciones {num_steps_episode}, reward {enviroment.total_reward}')\n",
    "            \n",
    "            \n",
    "    return episodes_list, best_episode\n",
    "\n",
    "\n",
    "def print_process_info(episodes_list, best_episode, print_best_episode_info=True,\n",
    "                       print_q_table=True, print_best_values_states=True,\n",
    "                       print_best_actions_states=True, print_steps=True, print_path=True):\n",
    "    \"\"\"\n",
    "    Método que imprime por pantalla los resultados de la ejecución\n",
    "    \"\"\"\n",
    "    if print_best_episode_info:\n",
    "        print(f\"\\n Mejor (último) Episodio: \\n Episodio {best_episode['Num_episode']}\\n \\t Número de acciones: {len(best_episode['episode'].actions_done)} \\n\\t Reward: {best_episode['episode'].total_reward}\")\n",
    "    \n",
    "    if print_q_table:\n",
    "        print('\\n Q_TABLE:')\n",
    "        best_episode['learner'].print_q_table()\n",
    "    \n",
    "    if print_best_values_states:\n",
    "        print('\\n Q_TABLE VALUES:')\n",
    "        best_episode['learner'].print_best_values_states()\n",
    "    \n",
    "    if print_best_actions_states:\n",
    "        print('\\n BEST ACTIONS:')\n",
    "        best_episode['learner'].print_best_actions_states()\n",
    "        \n",
    "    if print_steps:\n",
    "        print(f\"\\npasos: \\n {best_episode['episode'].actions_done}\")\n",
    "        \n",
    "    if print_path:\n",
    "        print('\\nPATH: ')\n",
    "        best_episode['episode'].print_path_episode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Política Aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1, Num Acciones 30, reward -30.0\n",
      "Episodio 2, Num Acciones 79, reward -79.0\n",
      "Episodio 3, Num Acciones 134, reward -134.0\n",
      "Episodio 4, Num Acciones 221, reward -221.0\n",
      "Episodio 5, Num Acciones 189, reward -189.0\n",
      "Episodio 6, Num Acciones 19, reward -19.0\n",
      "Episodio 7, Num Acciones 54, reward -54.0\n",
      "Episodio 8, Num Acciones 49, reward -49.0\n",
      "Episodio 9, Num Acciones 61, reward -61.0\n",
      "Episodio 10, Num Acciones 41, reward -41.0\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(Learner = Learner, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mejor (último) Episodio: \n",
      " Episodio 6\n",
      " \t Número de acciones: 19 \n",
      "\t Reward: -19.0\n",
      "\n",
      " Q_TABLE:\n",
      "Estado  Arriba  Abajo  Izquierda  Derecha\n",
      " x0,y0    0.00   0.00       0.00     0.00\n",
      " x0,y1    0.00   0.00       0.00     0.00\n",
      " x0,y2    0.00   0.00       0.00     0.00\n",
      " x0,y3    0.00   0.00       0.00     0.00\n",
      " x1,y0    0.00   0.00       0.00     0.00\n",
      " x1,y1    0.00   0.00       0.00     0.00\n",
      " x1,y2    0.00   0.00       0.00     0.00\n",
      " x1,y3    0.00   0.00       0.00     0.00\n",
      " x2,y0    0.00   0.00       0.00     0.00\n",
      " x2,y1    0.00   0.00       0.00     0.00\n",
      " x2,y2    0.00   0.00       0.00     0.00\n",
      " x2,y3    0.00   0.00       0.00     0.00\n",
      " x3,y0    0.00   0.00       0.00     0.00\n",
      " x3,y1    0.00   0.00       0.00     0.00\n",
      " x3,y2    0.00   0.00       0.00     0.00\n",
      " x3,y3    0.00   0.00       0.00     0.00\n",
      "\n",
      " Q_TABLE VALUES:\n",
      "     y0   y1   y2   y3\n",
      "x0 0.00 0.00 0.00 0.00\n",
      "x1 0.00 0.00 0.00 0.00\n",
      "x2 0.00 0.00 0.00 0.00\n",
      "x3 0.00 0.00 0.00 0.00\n",
      "\n",
      " BEST ACTIONS:\n",
      "        y0      y1      y2      y3\n",
      "x0  Arriba  Arriba  Arriba  Arriba\n",
      "x1  Arriba  Arriba  Arriba  Arriba\n",
      "x2  Arriba  Arriba  Arriba  Arriba\n",
      "x3  Arriba  Arriba  Arriba  Arriba\n",
      "\n",
      "pasos: \n",
      " [[1, 0], [2, 0], [1, 0], [1, 1], [2, 1], [2, 2], [3, 2], [3, 1], [3, 0], [2, 0], [1, 0], [1, 1], [1, 2], [1, 3], [1, 3], [1, 3], [2, 3], [2, 3], [3, 3]]\n",
      "\n",
      "PATH: \n",
      "    y0  y1  y2  y3\n",
      "x0   0   -   -   -\n",
      "x1  11  12  13  16\n",
      "x2  10   5   6  18\n",
      "x3   9   8   7  19\n"
     ]
    }
   ],
   "source": [
    " print_process_info(episodes_list, best_episode, print_best_episode_info=True,\n",
    "                       print_q_table=True, print_best_values_states=True,\n",
    "                       print_best_actions_states=True, print_steps=True, print_path=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Q-Learner: Implementación y Ejecución\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./Figures/013_qlearning.png\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'QLearner'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, is_final_state, **kwargs):\n",
    "        \"\"\"\n",
    "        Método que implementa el Algoritmo de aprendizaje del Q-Learning\n",
    "        :param environment:           Entorno en el que tomar las acciones\n",
    "        :param old_state:             Estado actual\n",
    "        :param action_taken:          Acción a realizar\n",
    "        :param reward_action_taken:   Recompensa obtenida por la acción tomada\n",
    "        :param new_state:             Nuevo estado al que se mueve el agente\n",
    "        :param is_final_state:        Boolean. Devuelve True si el agente llega al estado final; si no, False\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        # Obtenemos el identificador de la acción\n",
    "        \n",
    "\n",
    "        # Obtenemos el valor de la acción tomada\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a corto plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a largo plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## SARSA-Learner: Implementación y Ejecución\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./Figures/014_sarsa.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'SARSA'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, new_action, is_final_state):\n",
    "        \"\"\"\n",
    "        Método que implementa el algoritmo de aprendizaje SARSA\n",
    "        :param environment:           Entorno en el que tomar las acciones\n",
    "        :param old_state:             Estado actual\n",
    "        :param action_taken:          Acción a realizar\n",
    "        :param reward_action_taken:   Recompensa obtenida por la acción tomada\n",
    "        :param new_state:             Nuevo estado al que se mueve el agente \n",
    "        :param new_action:            Acción a tomar en el nuevo estado\n",
    "        :param is_final_state:        Boolean. Devuelve True si el agente llega al estado final; si no, False \n",
    "        \"\"\"\n",
    "        # Obtenemos el identificador de la acción\n",
    "        \n",
    "\n",
    "        # Obtenemos el valor de la acción tomada\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a corto plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a largo plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Estrategias a corto y largo plazo\n",
    "\n",
    "\n",
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
